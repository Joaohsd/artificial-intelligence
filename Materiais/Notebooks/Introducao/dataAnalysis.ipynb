{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0421eab7",
   "metadata": {},
   "source": [
    "# Análise de Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9349059",
   "metadata": {},
   "source": [
    "A **Análise de Dados** é um campo de pesquisa muito presente no mundo atual. Isso se da pelo fato do enorme número de dados coletados, os quais podem ser trabalhados a fim de gerar resultados ótimos. De fato, saber trabalhar com estes dados requer conhecimentos acerca de probabilidade, entre outras áreas. Dessa forma, saber definir dados úteis e dados a serem descartados podem acelerar o processo de treinamento do modelo de Machine Learning ao qual deseja-se treinar. Além disso, em inúmeros casos o redimensionamento da quantidade de atributos (features) pode facilitar tal processo, seja através do descarte ou pelo resumo destes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99426453",
   "metadata": {},
   "source": [
    "# **Tratando os Dados**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a156eb28",
   "metadata": {},
   "source": [
    "O **pré-processamento** dos dados é de suma importância para sua análise, uma vez que os dados foram tratados para serem utilizados durante o treinamento do modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496575e0",
   "metadata": {},
   "source": [
    "## **Normalização vs Padronização**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d5b7fb",
   "metadata": {},
   "source": [
    "Em muitos casos, é necessário fazer com que os dados tabalhem em uma mesma faixa de valores. De fato, o dimensionamento dos valores das Features é uma dasetapas de pré-processamento de dados mais importantes no aprendizado de máquina. Algoritmos que levam em consideração a distância entre essas features podem obeter resultados um tanto quanto altos, visto que os dados podem estar em faixas distintas de valores. Dessa forma, o dimensionamento de features ajuda o aprendizado de máquina e os algoritmos de aprendizado profundo a treinar e convergir mais rapidamente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c537cd",
   "metadata": {},
   "source": [
    "### **Normalização**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac3cdba",
   "metadata": {},
   "source": [
    "O objetivo da normalização dos dados é converter tal conjunto para uma escala de valores similar, ou seja, tranformá-las em uma mesma ordem de grandeza. Dessa maneira, os dados são convertidos para uma determinada faixa de valores, como: [0,1] ou [-1,1]. A fórmula mais utilizada para tal é mostrada abaixo, o qual intervalo assumido pela variável X será [0,1]:\n",
    "\n",
    "<center>${X}_{changed} = \\displaystyle \\Bigg[\\frac{X - {X}_{min}}{{X}_{max} - {X}_{min}}\\Bigg] $</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95804cc",
   "metadata": {},
   "source": [
    "### **Padronização**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a7c1bd",
   "metadata": {},
   "source": [
    "O objetivo da padronização é o mesmo da normalização, mas, neste caso, os dados gerados seguem a seguinte regra:\n",
    "\n",
    "*   Média igual à 0, ou seja, $\\overline{X} = 0$;\n",
    "*   Desvio padrão igual à 1, ou seja, $\\sigma = 1$;\n",
    "\n",
    "Tal método é muito aplicado para variáveis que seguem uma distribuição normal (Gaussiana) ou que apresentam um desvio padrão muito baixo. Seu resultado não possui um valor máximo ou mínimo. A fórmula utilizada para tal é:\n",
    "\n",
    "<center>${X}_{changed} = \\displaystyle \\Bigg[\\frac{X - \\overline{X}}{\\sigma}\\Bigg]$</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ea63ca",
   "metadata": {},
   "source": [
    "# **Redução de Dimensionalidade**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6792bab",
   "metadata": {},
   "source": [
    "A **Redução de Dimensionalidade** tem por objetivo reduzir o número de features do seu conjunto de features. Imagine um conjunto de dados com 100 colunas (features) e reduzimos tal para 20 colunas. Tal redução permite que se tenha dados resumidos, o que facilita o treinamento de um modelo de Machine Learning, visto este modelo sendo treinado com um grande conjunto de atributos tende a ser altamente dependente dos dados utilizados em seu treinamento, resultando em uma performance ruim com dados reais. Dessa maneira, quanto maior o número de features, mais complexo pode se tornar o modelo e mais tempo será necessário para o seu treinamento. Além disso, quanto maior o número de features, mais amostras são necessárias.\n",
    "\n",
    "<center><img src=\"img/featureComplexidade.png\" alt=\"Drawing\" style=\"width: 420px;height: 300px\"/></center>\n",
    "\n",
    "**Tal processo pode ser feito de várias maneiras, retirando features ou combinando-as para gerar novas features. Tais métodos são explicados abaixo:**\n",
    "\n",
    "<center><img src=\"img/selecaoFeatures.png\" alt=\"Drawing\" style=\"width: 550px;height: 350px\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6dfdff",
   "metadata": {},
   "source": [
    "## **Métodos para retirar features:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5118cf",
   "metadata": {},
   "source": [
    "Esse método mantém as features mais importante e descarta as que são redundantes do seu dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb9f4ea",
   "metadata": {},
   "source": [
    "### **Método de seleção de features:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e395c163",
   "metadata": {},
   "source": [
    "Este método tem por objetivo **identificar e selecionar os atributos que são relevantes** para o treinamento do modelo. Imagine que se deseja estimar o peso de uma pessoa, talvez, não seja interessante realizar o treinamento com a feature de cor da pele. Por outro lado, o atributo altura pode contribuir e muito para o treinamento.\n",
    "Tal processo pode ser feito manualmente, baseado na relevância identificada pela pessoa a qual está analisando o dataset. Ainda assim, quando isso não é possível, faz se uso de outras ferramentas, as quais são citadas abaixo:\n",
    "*    Utilizar um mapa de calor que mostre a correlação entre as features;\n",
    "*    Plotar um gráfico que relacione cada feature com a variável alvo;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0b3f17",
   "metadata": {},
   "source": [
    "Ainda é possível aplicar tal metodologia usando bibliotecas com recursos prontos, sendo uma delas a **sci-kit learn**. Um recurso dessa biblioteca é citado abaixo:\n",
    "\n",
    "*    **Limite de variação:** Tal método realiza o descarte das informações irrelevantes, baseado na variância a qual tal feature posssui. A partir disso, ele elimina as features, as quais sua própria variância não excede um determinado limite pré-estabelecido. Abaixo um exemplo, o qual descarta a primeira e a última coluna do dataset, visto que o limiar definido é **0**, pois não foi passado nada para o método \"VarianceThreshold()\". O limiar é definido pelo argumento passado para o método. Para mais informações sobre tal processo acesse o <a link=\"https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html\">link</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f082223",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "X = [[0, 2, 0, 3], [0, 1, 4, 3], [0, 1, 1, 3]]\n",
    "selector = VarianceThreshold()\n",
    "selector.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6f9b7d",
   "metadata": {},
   "source": [
    "Outro recurso da biblioteca do **sci-kit learn** é o:\n",
    "*    **Seleção Univariada:** A Seleção Univariada de features usa testes estatísticos para selecioná-las. Este método descreve um tipo de dado que consiste em observações em apenas uma única característica ou atributo. A seleção de feature univariado examina cada feature individualmente para determinar a força do relacionamento da feature com a variável de resposta. Alguns exemplos de testes estatísticos que podem ser usados para avaliar a relevância das características são Correlação de Pearson, Coeficiente de informação máxima, Correlação de distância, ANOVA e Chi-square. Chi-square é usado para encontrar a relação entre variáveis categóricas e Anova é preferida quando as variáveis são contínuas. Para mais informações sobre tal método, acesse o <a link=\"https://scikit-learn.org/stable/auto_examples/feature_selection/plot_feature_selection.html\">link</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23198898",
   "metadata": {},
   "source": [
    "## **Métodos para combinar features:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3ee6f2",
   "metadata": {},
   "source": [
    "### **Métodos lineares:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f229b2c",
   "metadata": {},
   "source": [
    "#### **Principal Component Analysis (PCA)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a058d3",
   "metadata": {},
   "source": [
    "**PCA** é uma técnica de redução de dimensionalidade linear (algoritmo) que transforma um conjunto de variáveis correlacionadas (p) em um número k (k<p) menor de variáveis não correlacionadas chamadas componentes principais, **mantendo o máximo possível da variação no conjunto de dados original**. No contexto do Machine Learning (ML), o PCA é um algoritmo de aprendizado de máquina não supervisionado que é usado para redução de dimensionalidade."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c004b0",
   "metadata": {},
   "source": [
    "#### **Factor Analysis (FA)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2279c9",
   "metadata": {},
   "source": [
    "**Factor Analysis (FA)** e **Principal Component Analysis (PCA)** são ambas técnicas de **redução de dimensionalidade**. O principal objetivo do Factor Analysis não é apenas reduzir a dimensionalidade dos dados. A análise fatorial é uma abordagem útil para encontrar variáveis latentes que não são medidas diretamente em uma única variável, mas sim inferidas de outras variáveis no conjunto de dados. Essas variáveis ​​latentes são chamadas de fatores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16e1094",
   "metadata": {},
   "source": [
    "#### **Linear Discriminant Analysis (LDA)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ea9bdc",
   "metadata": {},
   "source": [
    "O **LDA** é normalmente usado para **classificação multiclasse**. Também pode ser usado como uma técnica de redução de dimensionalidade. LDA melhor separa ou discrimina (daí o nome LDA) instâncias de treinamento por suas classes. A principal diferença entre o LDA e o PCA é que o LDA encontra uma combinação linear de recursos de entrada que otimiza a separação de classes enquanto o PCA tenta encontrar um conjunto de componentes não correlacionados de variação máxima em um conjunto de dados. Outra diferença importante entre os dois é que o PCA é um algoritmo não supervisionado, enquanto o LDA é um algoritmo supervisionado que leva em consideração os rótulos de classe.\n",
    "\n",
    "**LDA para redução de dimensionalidade não deve ser confundido com LDA para classificação multiclasse**. Ambos os casos podem ser implementados usando a função Scikit-learn LinearDiscriminantAnalysis(). Após ajustar o modelo usando fit(X, y), usamos o método predict(X) do objeto LDA para classificação multiclasse. Isso atribuirá novas instâncias às classes no conjunto de dados original. Podemos usar o método transform(X) do objeto LDA para redução de dimensionalidade. Isso encontrará uma combinação linear de novos recursos que otimiza a separação de classes.\n",
    "\n",
    "O código Python a seguir descreve a implementação das técnicas LDA e PCA para o conjunto de dados Iris e mostra a diferença entre os dois. O conjunto de dados original da Iris tem quatro recursos. LDA e PCA reduzem esse número de recursos em dois e permitem uma visualização 2D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2c8bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_scaled = sc.fit_transform(X)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "lda = LinearDiscriminantAnalysis(n_components=2, solver='svd')\n",
    "X_lda = lda.fit_transform(X, y)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(13.5 ,4))\n",
    "sns.scatterplot(X_pca[:,0], X_pca[:,1], hue=y, palette='Set1', ax=ax[0])\n",
    "sns.scatterplot(X_lda[:,0], X_lda[:,1], hue=y, palette='Set1', ax=ax[1])\n",
    "ax[0].set_title(\"PCA of IRIS dataset\", fontsize=15, pad=15)\n",
    "ax[1].set_title(\"LDA of IRIS dataset\", fontsize=15, pad=15)\n",
    "ax[0].set_xlabel(\"PC1\", fontsize=12)\n",
    "ax[0].set_ylabel(\"PC2\", fontsize=12)\n",
    "ax[1].set_xlabel(\"LD1\", fontsize=12)\n",
    "ax[1].set_ylabel(\"LD2\", fontsize=12)\n",
    "plt.savefig('PCA vs LDA.png', dpi=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6da0f17",
   "metadata": {},
   "source": [
    "#### **Truncated Singular Value Decomposition (SVD)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8e61e5",
   "metadata": {},
   "source": [
    "Este método realiza a redução da dimensionalidade linear por meio da decomposição de valor singular truncado (SVD). Funciona bem com dados esparsos em que muitos dos valores de linha são zero. Em contraste, o PCA funciona bem com dados densos. O SVD truncado também pode ser usado com dados densos. Outra diferença importante entre SVD truncado e PCA é que a fatoração para SVD é feita na matriz de dados, enquanto a fatoração para PCA é feita na matriz de covariância."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9093f426",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_scaled = sc.fit_transform(X)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "svd = TruncatedSVD(n_components=2, algorithm='randomized',\n",
    "                   random_state=0)\n",
    "X_svd = svd.fit_transform(X_scaled)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(13.5 ,4))\n",
    "sns.scatterplot(X_pca[:,0], X_pca[:,1], hue=y, palette='Set1', ax=ax[0])\n",
    "sns.scatterplot(X_svd[:,0], X_svd[:,1], hue=y, palette='Set1', ax=ax[1])\n",
    "ax[0].set_title(\"PCA of IRIS dataset\", fontsize=15, pad=15)\n",
    "ax[1].set_title(\"Truncated SVD of IRIS dataset\", fontsize=15, pad=15)\n",
    "ax[0].set_xlabel(\"PC1\", fontsize=12)\n",
    "ax[0].set_ylabel(\"PC2\", fontsize=12)\n",
    "ax[1].set_xlabel(\"SVD1\", fontsize=12)\n",
    "ax[1].set_ylabel(\"SVD2\", fontsize=12)\n",
    "plt.savefig('PCA vs SVD.png', dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8e9134",
   "metadata": {},
   "source": [
    "### **Métodos não-lineares:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd4df80",
   "metadata": {},
   "source": [
    "#### **Kernel PCA**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fb10e5",
   "metadata": {},
   "source": [
    "**Kernel PCA** é uma técnica de redução de dimensionalidade não linear que usa kernels. Também pode ser considerado como a forma não linear de PCA normal. Kernel PCA funciona bem com conjuntos de dados não lineares onde o PCA normal não pode ser usado com eficiência.\n",
    "\n",
    "A intuição por trás do Kernel PCA é algo interessante. Os dados são executados primeiro por meio de uma função do kernel e os projeta temporariamente em um novo espaço de recursos de dimensão superior, onde as classes se tornam linearmente separáveis (as classes podem ser divididas desenhando uma linha reta). Em seguida, o algoritmo usa o PCA normal para projetar os dados de volta em um espaço de menor dimensão. Desta forma, Kernel PCA transforma dados não lineares em um espaço de dados de menor dimensão que pode ser usado com classificadores lineares.\n",
    "\n",
    "No Kernel PCA, precisamos especificar 3 hiperparâmetros importantes — o número de componentes que queremos manter, o tipo de kernel e o coeficiente do kernel (também conhecido como **gama**). Para o tipo de kernel, podemos usar ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘cosine’. O kernel rbf, conhecido como kernel de função de base radial, é o mais popular."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3ef2dc",
   "metadata": {},
   "source": [
    "#### **Isometric mapping (Isomap)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0b2d10",
   "metadata": {},
   "source": [
    "Este método realiza redução de dimensionalidade não linear através de **mapeamento isométrico**. É uma extensão do MDS ou Kernel PCA. Ele conecta cada instância calculando a distância curva ou geodésica para seus vizinhos mais próximos e reduz a dimensionalidade. O número de vizinhos a serem considerados para cada ponto pode ser especificado através do hiperparâmetro n_neighbors da classe Isomap() que implementa o algoritmo Isomap no Scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0cba41",
   "metadata": {},
   "source": [
    "Para mais informações de métodos de redução de dimensionalidade, consulte o seguinte <a link=\"https://towardsdatascience.com/11-dimensionality-reduction-techniques-you-should-know-in-2021-dcb9500d388b\" >link</a>. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
